{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Topic Modeling\n",
    "\n",
    "This notebook demonstrates how to load and prepare text data for topic modeling. We'll work with the Cohere/movies dataset from Hugging Face.\n",
    "\n",
    "## Parameters Explanation\n",
    "\n",
    "- **dataset_type**: Source of our dataset (HF = Hugging Face)\n",
    "- **dataset_name**: The specific dataset we're using (Cohere/movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the type and name of the dataset\n",
    "dataset_type: str = \"HF\"\n",
    "dataset_name: str = \"Cohere/movies\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "We'll now load our dataset from Hugging Face and prepare it for topic modeling. This involves:\n",
    "1. Loading the raw text data\n",
    "2. Examining the data structure\n",
    "3. Handling any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariflor/TopicAnalyser/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4803"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.data_utils import TextPreProcessor, CorpusProcessor\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset (replace 'dataset_name' with the desired dataset's name from HuggingFace, or filename of the local dataset)\n",
    "if dataset_type == \"HF\":\n",
    "    dataset = load_dataset(dataset_name)\n",
    "else: dataset = load_dataset(\"csv\", data_files={\"train\": f\"{dataset_name}.csv\"})\n",
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4800\n"
     ]
    }
   ],
   "source": [
    "# Collect documents and print the number of documents\n",
    "documents = []\n",
    "for d in dataset['train']['overview']:\n",
    "    if d is not None:\n",
    "        documents.append(d)\n",
    "\n",
    "print('Number of documents:', len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 documents: ['In the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but becomes torn between following orders and protecting an alien civilization.', '84 years later, a 101-year-old woman named Rose DeWitt Bukater tells the story to her granddaughter Lizzy Calvert, Brock Lovett, Lewis Bodine, Bobby Buell and Anatoly Mikailavich on the Keldysh about her life set in April 10th 1912, on a ship called Titanic when young Rose boards the departing ship with the upper-class passengers and her mother, Ruth DeWitt Bukater, and her fianc√©, Caledon Hockley. Meanwhile, a drifter and artist named Jack Dawson and his best friend Fabrizio De Rossi win third-class tickets to the ship in a game. And she explains the whole story from departure until the death of Titanic on its first and last voyage April 15th, 1912 at 2:20 in the morning.']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 2 documents to get an idea of the data\n",
    "print('First 2 documents:', documents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Before running our topic model, we need to clean and prepare the text data:\n",
    "1. Tokenization: Breaking text into individual words\n",
    "2. Removing stopwords: Common words like \"the\", \"and\", \"is\" that don't carry much meaning\n",
    "3. Lemmatization/stemming: Reducing words to their root forms\n",
    "4. Creating a document-term matrix: Converting processed text to numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 preprocessed documents: [['in', 'the', 'nd', 'century', 'a', 'paraplegic', 'marine', 'is', 'dispatched', 'to', 'the', 'moon', 'pandora', 'on', 'a', 'unique', 'mission', 'but', 'becomes', 'torn', 'between', 'following', 'orders', 'and', 'protecting', 'an', 'alien', 'civilization'], ['years', 'later', 'a', 'year', 'old', 'woman', 'named', 'rose', 'dewitt', 'bukater', 'tells', 'the', 'story', 'to', 'her', 'granddaughter', 'lizzy', 'calvert', 'brock', 'lovett', 'lewis', 'bodine', 'bobby', 'buell', 'and', 'anatoly', 'mikailavich', 'on', 'the', 'keldysh', 'about', 'her', 'life', 'set', 'in', 'april', 'th', 'on', 'a', 'ship', 'called', 'titanic', 'when', 'young', 'rose', 'boards', 'the', 'departing', 'ship', 'with', 'the', 'upper', 'class', 'passengers', 'and', 'her', 'mother', 'ruth', 'dewitt', 'bukater', 'and', 'her', 'fianc', 'caledon', 'hockley', 'meanwhile', 'a', 'drifter', 'and', 'artist', 'named', 'jack', 'dawson', 'and', 'his', 'best', 'friend', 'fabrizio', 'de', 'rossi', 'win', 'third', 'class', 'tickets', 'to', 'the', 'ship', 'in', 'a', 'game', 'and', 'she', 'explains', 'the', 'whole', 'story', 'from', 'departure', 'until', 'the', 'death', 'of', 'titanic', 'on', 'its', 'first', 'and', 'last', 'voyage', 'april', 'th', 'at', 'in', 'the', 'morning']]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text documents\n",
    "tp = TextPreProcessor()\n",
    "documents = tp.preprocess(documents)\n",
    "\n",
    "# Print the first 2 preprocessed documents\n",
    "print('First 2 preprocessed documents:', documents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per document: 53.24791666666667\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average number of words in a document\n",
    "average_words = np.mean([len(d) for d in documents])\n",
    "print('Average number of words per document:', average_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4270\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Process the documents to create a document-term matrix\n",
    "cp = CorpusProcessor(max_relative_frequency=0.9, min_absolute_frequency=5)\n",
    "cp.process(documents)\n",
    "\n",
    "# Get and save the vocabulary\n",
    "vocab = cp.get_vocab()\n",
    "with open('./data/vocab.pkl', 'wb') as file:\n",
    "    pickle.dump(vocab, file)\n",
    "print('Vocabulary size:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix shape: (4799, 4270)\n"
     ]
    }
   ],
   "source": [
    "# Get the document-term matrix and save it\n",
    "X = cp.get_vectorised_documents()\n",
    "with open('./data/doc_term_matrix.pkl', 'wb') as file:\n",
    "    pickle.dump(X, file)\n",
    "print('Document-term matrix shape:', X.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
